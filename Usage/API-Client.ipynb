{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CDG - three-tier-geolocation - REST API Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "REST_URL = \"http://127.0.0.1:5000\"\n",
    "\n",
    "DATASETS_PATH = \"..//Datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUESTED_FILE_NAME = \"results.txt\" \n",
    "\n",
    "def use_api(dataset_1p, dataset_3p, rtt_method, geolocation_method):\n",
    "    output_file = os.path.join(OUTPUT_PATH, f\"results_learn={dataset_1p}_test={dataset_3p}.txt\")\n",
    "    \n",
    "    print(\"start:\", dataset_1p, dataset_3p)\n",
    "    \n",
    "    dataset_1p_path = os.path.join(DATASETS_PATH, dataset_1p)\n",
    "    dataset_3p_path = os.path.join(DATASETS_PATH, dataset_3p)\n",
    "    \n",
    "    FILE_MEASUREMENTS_1P = os.path.join(dataset_1p_path, \"measurements-1party.csv\")\n",
    "    FILE_MEASUREMENTS_3P = os.path.join(dataset_3p_path, \"measurements-3party.csv\")\n",
    "    FILE_SERVERS_1P = os.path.join(dataset_1p_path, \"servers-1party.csv\")\n",
    "    FILE_SERVERS_3P = os.path.join(dataset_3p_path, \"servers-3party.csv\")\n",
    "    FILE_DATACENTERS = os.path.join(dataset_3p_path, \"datacenters.csv\")\n",
    "    FILE_SOLUTION = os.path.join(dataset_3p_path, \"solution.csv\")\n",
    "\n",
    "    response = requests.post(REST_URL + \"/rest\",\n",
    "                             files=[\n",
    "                                 ('measurements-1party', ('measurements-1party.csv', open(FILE_MEASUREMENTS_1P, 'rb'), 'application/octet-stream')),\n",
    "                                 ('servers-1party', ('servers-1party.csv', open(FILE_SERVERS_1P, 'rb'), 'application/octet-stream')),\n",
    "                                 ('measurements-3party', ('measurements-3party.csv', open(FILE_MEASUREMENTS_3P, 'rb'), 'application/octet-stream')),\n",
    "                                 ('servers-3party', ('servers-3party.csv', open(FILE_SERVERS_3P, 'rb'), 'application/octet-stream')),\n",
    "                                 ('datacenters', ('datacenters.csv', open(FILE_DATACENTERS, 'rb'), 'application/octet-stream')),\n",
    "                                 ('solution', ('solution.csv', open(FILE_SOLUTION, 'rb'), 'application/octet-stream'))\n",
    "                             ],\n",
    "                             data={'rtt_method': rtt_method, 'geolocation_method': geolocation_method})\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(\"Error: POST received bad response code:\", response.status_code)\n",
    "        return\n",
    "\n",
    "    requested_file_url = response.json()['Assets'][REQUESTED_FILE_NAME]\n",
    "    print(requested_file_url)\n",
    "\n",
    "    second_response = requests.get(requested_file_url)\n",
    "\n",
    "    if second_response.status_code != 200:\n",
    "        print(\"Error: GET received bad response code:\", response.status_code)\n",
    "        return\n",
    "\n",
    "    with open(output_file, 'w', encoding=\"utf-8\") as f:\n",
    "        f.writelines(second_response.text)\n",
    "    return second_response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtt_method = \"Optimization\"  # rtt_method = \"Subtraction\", \"Optimization\"\n",
    "geolocation_method = \"Multilateration\"  # geolocation_method = \"Multilateration\", \"Fingerprinting\"\n",
    "OUTPUT_PATH = \"./api-client-out/temp\"  # make sure this directory exists\n",
    "use_api(dataset_1p=\"DS-F3\", dataset_3p=\"DS-F4\", rtt_method=rtt_method, geolocation_method=geolocation_method)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple requests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtt_method = \"Optimization\"  # rtt_method = \"Subtraction\", \"Optimization\"\n",
    "geolocation_method = \"Fingerprinting\"  # geolocation_method = \"Multilateration\", \"Fingerprinting\"\n",
    "OUTPUT_PATH = f\"api-client-out/{rtt_method}_{geolocation_method}\"  # make sure this directory exists\n",
    "\n",
    "DATASETS = [\"DS-F1\", \"DS-F2\", \"DS-F3\", \"DS-F4\", ]  # \"DS-B1\"\n",
    "\n",
    "for d1 in DATASETS:\n",
    "    for d2 in DATASETS:\n",
    "        use_api(dataset_1p=d1, dataset_3p=d2, rtt_method=rtt_method, geolocation_method=geolocation_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_stats(text):\n",
    "    mean_error = 0\n",
    "    max_error = 0\n",
    "    RMSE = 0\n",
    "    success = 0\n",
    "    for line in text.splitlines():\n",
    "        if \"Final-Geolocation Mean Error:\" in line:\n",
    "            parts = line.split()\n",
    "            mean_error = parts[-2]\n",
    "        elif \"Final-Geolocation Max Error:\" in line:\n",
    "            parts = line.split()\n",
    "            max_error = parts[-2]\n",
    "\n",
    "        elif \"Final-Geolocation RMSE Error:\" in line:\n",
    "            parts = line.split()\n",
    "            RMSE = parts[-2]\n",
    "\n",
    "        elif \"Success rate:\" in line:\n",
    "            parts = line.split()\n",
    "            success  = parts[-2] \n",
    "    return mean_error, max_error, RMSE, success\n",
    "\n",
    "def clear_directories(path):\n",
    "    for filename in os.listdir(path):\n",
    "        file_path = os.path.join(path, filename)\n",
    "        os.remove(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use one DC in each continent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from itertools import product\n",
    "import time \n",
    "\n",
    "rtt_method = \"Optimization\"  # rtt_method = \"Subtraction\", \"Optimization\"\n",
    "geolocation_method = \"Multilateration\"  # geolocation_method = \"Multilateration\", \"Fingerprinting\"\n",
    "OUTPUT_PATH = \"./api-client-out/temp\"  # make sure this directory exists\n",
    "\n",
    "\n",
    "def filter_first_party(combination, first_party):\n",
    "    filtered_rows = []\n",
    "    file_names = []\n",
    "    first_party_servers = f\"..//Datasets//{first_party}//servers-1party.csv\"\n",
    "    first_party_measurments = f\"..//Datasets//{first_party}//measurements-1party.csv\"\n",
    "    with open(first_party_servers, mode='r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader: # filter the servers (only keep the append to disable)\n",
    "            if len(row) > 2 and row[0] == \"file\" and row[2] not in combination:\n",
    "                file_names.append(row[1])\n",
    "                filtered_rows.append(row)\n",
    "            else:\n",
    "                filtered_rows.append(row)\n",
    "    # make sure the dir exists\n",
    "    with open('../Datasets/temp-first/servers-1party.csv', mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(filtered_rows)\n",
    "    filtered_rows = []\n",
    "    #filter the measurements (only keep the append to disable)\n",
    "    with open(first_party_measurments, mode='r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            # if row[2] not in file_names: \n",
    "            filtered_rows.append(row)\n",
    "    with open('../Datasets/temp-first/measurements-1party.csv', mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(filtered_rows)     \n",
    "    return file_names\n",
    "\n",
    "def filter_third_party_for_one_each(third_party, combination):\n",
    "    filtered_rows = []\n",
    "    third_party_servers = f\"..//Datasets//{third_party}//servers-3party.csv\"\n",
    "    third_party_measurments = f\"..//Datasets//{third_party}//measurements-3party.csv\"\n",
    "    third_party_DC = f\"..//Datasets//{third_party}//datacenters.csv\"\n",
    "    third_party_solution = f\"..//Datasets//{third_party}//solution.csv\"\n",
    "    first_party_servers = f\"..//Datasets//{third_party}//servers-1party.csv\"\n",
    "    file_names = []\n",
    "\n",
    "    with open(first_party_servers, mode='r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader: # filter the servers (only keep the append to disable)\n",
    "            if len(row) > 2 and row[0] == \"file\" and row[2] not in combination:\n",
    "                file_names.append(row[1])\n",
    "                filtered_rows.append(row)\n",
    "            else:\n",
    "                filtered_rows.append(row)\n",
    "\n",
    "    with open(third_party_servers, mode='r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader: # filter the servers\n",
    "            if len(row) > 1 and row[0] == \"file\" and row[1] in file_names:\n",
    "                pass\n",
    "            else:\n",
    "                filtered_rows.append(row)\n",
    "        with open('../Datasets/temp-third/servers-3party.csv', mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(filtered_rows)\n",
    "    filtered_rows = []\n",
    "    with open(third_party_measurments, mode='r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader: # filter the measurments\n",
    "            if row[2] not in file_names:\n",
    "                filtered_rows.append(row)\n",
    "        with open('../Datasets/temp-third/measurements-3party.csv', mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(filtered_rows)\n",
    "    filtered_rows = []\n",
    "    with open(third_party_DC, mode='r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader: # add learn_only as needed\n",
    "            if row[0] not in combination:\n",
    "                row[-1] = \"learn_only\"\n",
    "                row.append(\"\")\n",
    "            filtered_rows.append(row)\n",
    "    with open('../Datasets/temp-third/datacenters.csv', mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(filtered_rows)\n",
    "    filtered_rows = []\n",
    "    with open(third_party_solution,mode ='r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader: # filter the solution\n",
    "            if row[0] not in file_names: \n",
    "                filtered_rows.append(row)\n",
    "    with open('../Datasets/temp-third/solution.csv', mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(filtered_rows)\n",
    "\n",
    "def cycle_DC(dataset_1p=\"DS-F3\", dataset_3p=\"DS-F4\"):\n",
    "    datacenters = {}\n",
    "    with open(f'..//Datasets//{dataset_3p}//datacenters.csv', mode='r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            name = row[0]\n",
    "            continent = row[3]\n",
    "            if continent not in datacenters:\n",
    "                datacenters[continent] = []\n",
    "            datacenters[continent].append(name)\n",
    "    continents = list(datacenters.keys())\n",
    "    combinations = list(product(*[datacenters[continent] for continent in continents]))\n",
    "    results = {}\n",
    "    for rtt_method in [\"Subtraction\"]:\n",
    "        for geolocation_method in [\"Fingerprinting\"]:\n",
    "            total_combinations = 0\n",
    "            total_mean_error = 0\n",
    "            total_max_error = 0\n",
    "            total_RMSE = 0\n",
    "            total_success = [0,0]\n",
    "            for combination in combinations:\n",
    "                total_combinations += 1\n",
    "                # file_names = filter_first_party(combination, dataset_1p)\n",
    "                filter_third_party_for_one_each(dataset_3p, combination)\n",
    "                text = use_api(dataset_1p=dataset_1p, dataset_3p=\"temp-third\", rtt_method=rtt_method, geolocation_method=geolocation_method)\n",
    "                mean_error, max_error, RMSE, success = text_to_stats(text)\n",
    "                total_mean_error += float(mean_error)\n",
    "                total_max_error += float(max_error)\n",
    "                total_RMSE += float(RMSE)\n",
    "                total_success[0] += int(success.split(\"/\")[0])\n",
    "                total_success[1] += int(success.split(\"/\")[1])\n",
    "                clear_directories(\"../Datasets/temp-first\")\n",
    "                clear_directories(\"../Datasets/temp-third\")\n",
    "            total_mean_error /= total_combinations\n",
    "            total_max_error /= total_combinations\n",
    "            total_RMSE /= total_combinations\n",
    "            results.update({(rtt_method,geolocation_method):[total_mean_error, total_max_error, total_RMSE, total_success]})\n",
    "    return results\n",
    "start_time = time.time()\n",
    "results = cycle_DC()\n",
    "end_time = time.time()\n",
    "print(f'Total Time: \"{end_time-start_time}')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying number of front-end servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rtt_method = \"Subtraction\"  # rtt_method = \"Subtraction\", \"Optimization\"\n",
    "geolocation_method = \"Fingerprinting\"  # geolocation_method = \"Multilateration\", \"Fingerprinting\"\n",
    "OUTPUT_PATH = \"./api-client-out/temp\"  # make sure this directory exists\n",
    "\n",
    "\n",
    "TEMP_DATASET_PATH = DATASETS_PATH + \"/temp-third\"\n",
    "\n",
    "if not os.path.exists(TEMP_DATASET_PATH):\n",
    "    os.makedirs(TEMP_DATASET_PATH)\n",
    "\n",
    "\n",
    "def filter_third_party_for_changing_frontend(third_party, n):\n",
    "    third_party_servers = f\"..//Datasets//{third_party}//servers-3party.csv\"\n",
    "    third_party_measurments = f\"..//Datasets//{third_party}//measurements-3party.csv\"\n",
    "    third_party_DC = f\"..//Datasets//{third_party}//datacenters.csv\"\n",
    "    third_party_solution = f\"..//Datasets//{third_party}//solution.csv\"\n",
    "    first_party_servers = f\"..//Datasets//{third_party}//servers-1party.csv\"\n",
    "    frontend_elements = []\n",
    "    with open(third_party_servers, newline='') as csvfile:\n",
    "        non_frontend_lines = []\n",
    "        reader = csv.reader(csvfile)\n",
    "        lines = list(reader)\n",
    "        frontend_lines = [(i, line) for i, line in enumerate(lines) if len(line) > 1 and line[0] == \"frontend\"]\n",
    "        selected_frontend_lines = random.sample(frontend_lines, min(n, len(frontend_lines)))\n",
    "        selected_indices = {index for index, _ in selected_frontend_lines}\n",
    "        final_lines = []\n",
    "        selected_frontend_elements = []\n",
    "        for i, line in enumerate(lines):\n",
    "            if i in selected_indices:\n",
    "                final_lines.append(line)\n",
    "                frontend_elements.append(line[1])  # Capture the second element\n",
    "            elif i not in [tup[0] for tup in frontend_lines]:\n",
    "                final_lines.append(line)\n",
    "\n",
    "        with open(TEMP_DATASET_PATH + '\\\\servers-3party.csv', mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(final_lines)\n",
    "\n",
    "    with open(third_party_measurments, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        frontend_lines = []\n",
    "        \n",
    "        for row in reader:\n",
    "            if row[1] in frontend_elements:\n",
    "                frontend_lines.append(row)\n",
    "\n",
    "        with open(TEMP_DATASET_PATH + '\\\\measurements-3party.csv', mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(frontend_lines)\n",
    "\n",
    "    with open(third_party_DC, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        frontend_lines = []\n",
    "        for row in reader:\n",
    "            frontend_lines.append(row)\n",
    "\n",
    "        with open(TEMP_DATASET_PATH + '\\\\datacenters.csv', mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(frontend_lines)\n",
    "\n",
    "    with open(third_party_solution, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        \n",
    "        for row in reader:\n",
    "            frontend_lines.append(row)\n",
    "\n",
    "        with open(TEMP_DATASET_PATH + '\\\\solution.csv', mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(frontend_lines)\n",
    "        \n",
    "def changing_number_of_frontend(dataset_1p, dataset_3p,num_iteration = 100):\n",
    "    results_per_n = {}\n",
    "    for n in range(3,20): # 19 datacenters overall\n",
    "        total_mean_error = 0\n",
    "        total_max_error = 0\n",
    "        total_RMSE = 0\n",
    "        total_success = [0,0]\n",
    "        for iteration in range(num_iteration):\n",
    "            filter_third_party_for_changing_frontend(dataset_3p,n)\n",
    "            text = use_api(dataset_1p=dataset_1p, dataset_3p=\"temp-third\", rtt_method=rtt_method, geolocation_method=geolocation_method)\n",
    "            mean_error, max_error, RMSE, success = text_to_stats(text)\n",
    "            total_mean_error += float(mean_error)\n",
    "            total_max_error += float(max_error)\n",
    "            total_RMSE += float(RMSE)\n",
    "            total_success[0] += int(success.split(\"/\")[0])\n",
    "            total_success[1] += int(success.split(\"/\")[1])\n",
    "        total_mean_error /= num_iteration\n",
    "        total_max_error /= num_iteration\n",
    "        total_RMSE /= num_iteration\n",
    "        results_per_n.update({(rtt_method,geolocation_method, n):[total_mean_error, total_max_error, total_RMSE, total_success]})\n",
    "    return results_per_n\n",
    "\n",
    "\n",
    "def save_results_to_csv(results, filename):\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"rtt_method\", \"geolocation_method\", \"n\", \"mean_error\", \"max_error\", \"RMSE\", \"success\"])\n",
    "        for key, value in results.items():\n",
    "            rtt_method, geolocation_method, n = key\n",
    "            mean_error, max_error, RMSE, success = value\n",
    "            writer.writerow([rtt_method, geolocation_method, n, mean_error, max_error, RMSE, success])\n",
    "\n",
    "\n",
    "dataset_1p=\"DS-F3\"\n",
    "dataset_3p=\"DS-F4\"\n",
    "results = changing_number_of_frontend(dataset_1p, dataset_3p)\n",
    "save_results_to_csv(results, f\"output/results_{dataset_1p}_{dataset_3p}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plots\n",
    "\n",
    "\n",
    "def load_results_from_csv(filename):\n",
    "    results = {}\n",
    "    with open(filename, mode='r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)  # skip the header\n",
    "        for row in reader:\n",
    "            rtt_method, geolocation_method, n, mean_error, max_error, RMSE, success = row\n",
    "            success = eval(success)  # convert string representation of list to actual list\n",
    "            results.update({(rtt_method, geolocation_method, int(n)): [float(mean_error), float(max_error), float(RMSE), success]})\n",
    "    return results\n",
    "\n",
    "dataset_1p=\"DS-F2\"\n",
    "dataset_3p=\"DS-F1\"\n",
    "results = load_results_from_csv(f\"output/results_{dataset_1p}_{dataset_3p}.csv\")\n",
    "\n",
    "n_values = []\n",
    "mean_errors = []\n",
    "max_errors = []\n",
    "RMSEs = []\n",
    "successes = []\n",
    "for key, value in results.items():\n",
    "    n = key[2]\n",
    "    total_mean_error = value[0]\n",
    "    mean_errors.append(total_mean_error)\n",
    "    total_max_error = value[1]\n",
    "    max_errors.append(total_max_error)\n",
    "    total_RMSE = value[2]\n",
    "    RMSEs.append(total_RMSE)\n",
    "    total_success = value[3][0] / value[3][1]\n",
    "    successes.append(total_success)\n",
    "    n_values.append(n)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_values, mean_errors, marker='o', linestyle='-', label = \"Average Mean Error\")\n",
    "plt.plot(n_values, max_errors, marker='o', linestyle='-', label = \"Average Max Error\")\n",
    "plt.plot(n_values, RMSEs, marker='o', linestyle='-', label = \"Average RMSE\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.legend()\n",
    "plt.title('Average Errors vs. Number of Front-end Servers', fontsize=24)\n",
    "plt.xlabel('Number of Front-end Servers', fontsize=22)\n",
    "plt.ylabel('Error [km]', fontsize=22)\n",
    "plt.xticks(n_values)\n",
    "plt.grid(True)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_values, successes, marker='o', linestyle='-', label = \"Average success\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Average Success Rate vs. Number of Front-end Servers', fontsize=24)\n",
    "plt.xlabel('Number of Front-end Servers', fontsize=22)\n",
    "plt.ylabel('Average Success Rate', fontsize=22)\n",
    "plt.xticks(n_values)\n",
    "plt.grid(True)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
